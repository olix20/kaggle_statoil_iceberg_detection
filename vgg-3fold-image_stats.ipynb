{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "7922e149-72fa-48cd-a545-b70bddb40d28",
    "_uuid": "9341f2516086ca38bca96e06a9dbfc39f813a95a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils \n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "9d5839b6-f7fb-426d-b05f-cf2dba9313a8",
    "_uuid": "d375d4f754ad7fb77db2142c7c075b4ad4168390"
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"data/input/train.json\")\n",
    "target_train=train['is_iceberg']\n",
    "test = pd.read_json(\"data/input/test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(train,label=1):\n",
    "    train['max'+str(label)] = [np.max(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "#     train['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "#     train['min'+str(label)] = [np.min(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "#     train['minpos'+str(label)] = [np.argmin(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "#     train['med'+str(label)] = [np.median(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "#     train['std'+str(label)] = [np.std(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "#     train['mean'+str(label)] = [np.mean(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "#     train['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in train['band_'+str(label)] ]\n",
    "#     train['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in train['band_'+str(label)] ]\n",
    "#     train['mid50_'+str(label)] = train['p75_'+str(label)]-train['p25_'+str(label)]\n",
    "\n",
    "    return train\n",
    "train = get_stats(train,1)\n",
    "train = get_stats(train,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.12 s, sys: 0 ns, total: 3.12 s\n",
      "Wall time: 3.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "test = get_stats(test,1)\n",
    "test = get_stats(test,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'band_1', u'band_2', u'id', u'inc_angle', u'is_iceberg', u'max1',\n",
       "       u'max2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1604, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:,3:].drop(['is_iceberg'],axis=1).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'band_1', u'band_2', u'id', u'inc_angle', u'max1', u'max2'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[:,3:].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['inc_angle',u'max1',u'maxpos1',u'min1',u'minpos1',u'med1', u'std1', u'mean1',u'p25_1',u'p75_1',  u'mid50_1'  ]\n",
    "X_angle= train.iloc[:,3:].drop(['is_iceberg'],axis=1).values\n",
    "X_test_angle= test.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weights = {0:}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1053.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[0]/8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_band_3=(X_band_1+X_band_2)/2.\n",
    "#X_band_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in train[\"inc_angle\"]])\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n",
    "                          , X_band_2[:, :, :, np.newaxis]\n",
    "                         , X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_band_test_3=(X_band_test_1+X_band_test_2)/2.\n",
    "#X_band_test_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in test[\"inc_angle\"]])\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , X_band_test_3[:, :, :, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size=64\n",
    "# Define the image transformations here\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.,\n",
    "                         height_shift_range = 0.,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range = 0.2,\n",
    "                         rotation_range = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "def get_callbacks(filepath, patience=5):\n",
    "    es = EarlyStopping('val_loss', patience=5, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "#     lrplateu = ReduceLROnPlateau(monitor='val_loss',  patience=2, verbose=1, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "    return [es, msave]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    input_2 = Input(shape=[X_angle.shape[1]], name=\"angle\")\n",
    "    angle_layer = Dense(2, )(input_2)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, \n",
    "                 input_shape=X_train.shape[1:], classes=1)\n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "\n",
    "    x = GlobalMaxPooling2D()(x) #\n",
    "    merge_one = concatenate([x, angle_layer])\n",
    "    merge_one = Dense(512, activation='relu', name='fc2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=[base_model.input, input_2], output=predictions)\n",
    "    \n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "067f3dd7-3dcf-4b71-857d-e00b4afbd06e",
    "_uuid": "af8be6ce23dba815bbde23fd7e196eb54ae7c4e1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Using K-fold Cross Validation with Data Augmentation.\n",
    "def trainKfold(X_train, X_angle, X_test):\n",
    "    K=3\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=17).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    \n",
    "    \n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j+1)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "        #Angle\n",
    "        X_angle_cv=X_angle[train_idx]\n",
    "        X_angle_hold=X_angle[test_idx]\n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"weights/{}_{}.hdf5\".format(exp_name,j+1)\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv, y_train_cv)\n",
    "        \n",
    "        model= getModel()\n",
    "        model.fit_generator(\n",
    "                gen_flow,\n",
    "                steps_per_epoch=24,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=([X_holdout,X_angle_hold], Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        model.load_weights(filepath=file_path)\n",
    "        #Getting Training Score\n",
    "        score = model.evaluate([X_train_cv,X_angle_cv], y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        \n",
    "        #Getting Test Score\n",
    "        score = model.evaluate([X_holdout,X_angle_hold], Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.\n",
    "        pred_valid=model.predict([X_holdout,X_angle_hold])\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "\n",
    "        temp_test=model.predict([X_test, X_test_angle])\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "        \n",
    "\n",
    "\n",
    "        #Getting Train Scores\n",
    "        temp_train=model.predict([X_train, X_angle])\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "\n",
    "    \n",
    "    train_log_loss = log_loss(target_train, y_train_pred_log) \n",
    "    valid_log_loss = log_loss(target_train, y_valid_pred_log)\n",
    "    \n",
    "    print('\\n Train Log Loss Validation= ',train_log_loss)\n",
    "    print(' Valid Log Loss Validation= ',valid_log_loss)\n",
    "    \n",
    "    \n",
    "    return y_train_pred_log, y_valid_pred_log, y_test_pred_log, train_log_loss, valid_log_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n===================FOLD=', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:18: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 20s - loss: 0.6480 - acc: 0.6490 - val_loss: 0.4320 - val_acc: 0.7813\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 13s - loss: 0.3868 - acc: 0.8012 - val_loss: 0.2811 - val_acc: 0.8729\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 13s - loss: 0.3371 - acc: 0.8370 - val_loss: 0.2550 - val_acc: 0.8841\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 13s - loss: 0.3063 - acc: 0.8671 - val_loss: 0.2241 - val_acc: 0.9028\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 13s - loss: 0.2620 - acc: 0.8913 - val_loss: 0.2239 - val_acc: 0.8972\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 13s - loss: 0.2615 - acc: 0.8937 - val_loss: 0.2307 - val_acc: 0.9047\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 13s - loss: 0.2492 - acc: 0.8928 - val_loss: 0.2379 - val_acc: 0.9047\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 13s - loss: 0.2485 - acc: 0.8940 - val_loss: 0.2335 - val_acc: 0.9028\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 13s - loss: 0.2197 - acc: 0.9067 - val_loss: 0.2395 - val_acc: 0.8991\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 13s - loss: 0.2347 - acc: 0.9035 - val_loss: 0.2417 - val_acc: 0.8972\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 13s - loss: 0.2004 - acc: 0.9178 - val_loss: 0.2206 - val_acc: 0.9028\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 13s - loss: 0.1994 - acc: 0.9096 - val_loss: 0.2224 - val_acc: 0.9103\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 13s - loss: 0.1849 - acc: 0.9236 - val_loss: 0.2065 - val_acc: 0.9047\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 13s - loss: 0.1972 - acc: 0.9129 - val_loss: 0.2324 - val_acc: 0.9084\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 13s - loss: 0.2185 - acc: 0.9095 - val_loss: 0.2014 - val_acc: 0.9252\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 13s - loss: 0.1842 - acc: 0.9226 - val_loss: 0.2066 - val_acc: 0.9234\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 13s - loss: 0.1909 - acc: 0.9169 - val_loss: 0.2316 - val_acc: 0.9028\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 13s - loss: 0.1721 - acc: 0.9282 - val_loss: 0.1954 - val_acc: 0.9234\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 13s - loss: 0.1654 - acc: 0.9361 - val_loss: 0.2190 - val_acc: 0.9234\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 13s - loss: 0.1648 - acc: 0.9321 - val_loss: 0.2089 - val_acc: 0.9121\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 13s - loss: 0.1316 - acc: 0.9445 - val_loss: 0.2343 - val_acc: 0.9178\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 13s - loss: 0.1601 - acc: 0.9381 - val_loss: 0.2353 - val_acc: 0.9121\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 13s - loss: 0.1310 - acc: 0.9494 - val_loss: 0.2347 - val_acc: 0.9140\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 13s - loss: 0.1261 - acc: 0.9504 - val_loss: 0.2210 - val_acc: 0.9215\n",
      "('Train loss:', 0.12518729558745983)\n",
      "('Train accuracy:', 0.95509822263797939)\n",
      "('Test loss:', 0.19538722768008152)\n",
      "('Test accuracy:', 0.92336448653836112)\n",
      "('\\n===================FOLD=', 2)\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 15s - loss: 0.6923 - acc: 0.6551 - val_loss: 0.3608 - val_acc: 0.8355\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 13s - loss: 0.3988 - acc: 0.8158 - val_loss: 0.2941 - val_acc: 0.8766\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 13s - loss: 0.3110 - acc: 0.8668 - val_loss: 0.2781 - val_acc: 0.8748\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 13s - loss: 0.2846 - acc: 0.8804 - val_loss: 0.2638 - val_acc: 0.8991\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 13s - loss: 0.2431 - acc: 0.9024 - val_loss: 0.2768 - val_acc: 0.8879\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 13s - loss: 0.2392 - acc: 0.9062 - val_loss: 0.2706 - val_acc: 0.8841\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 13s - loss: 0.2293 - acc: 0.9020 - val_loss: 0.2515 - val_acc: 0.8953\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 13s - loss: 0.2280 - acc: 0.8999 - val_loss: 0.2794 - val_acc: 0.8972\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 13s - loss: 0.1876 - acc: 0.9220 - val_loss: 0.2680 - val_acc: 0.9009\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 13s - loss: 0.2192 - acc: 0.9169 - val_loss: 0.2820 - val_acc: 0.8953\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 13s - loss: 0.1813 - acc: 0.9279 - val_loss: 0.3014 - val_acc: 0.8766\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 13s - loss: 0.1888 - acc: 0.9158 - val_loss: 0.2487 - val_acc: 0.8953\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 13s - loss: 0.1898 - acc: 0.9300 - val_loss: 0.2775 - val_acc: 0.8785\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 13s - loss: 0.1798 - acc: 0.9335 - val_loss: 0.2536 - val_acc: 0.9065\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 13s - loss: 0.1861 - acc: 0.9199 - val_loss: 0.2487 - val_acc: 0.8860\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 13s - loss: 0.1647 - acc: 0.9316 - val_loss: 0.2306 - val_acc: 0.8972\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 13s - loss: 0.1555 - acc: 0.9355 - val_loss: 0.2661 - val_acc: 0.8916\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 13s - loss: 0.1833 - acc: 0.9288 - val_loss: 0.2586 - val_acc: 0.8953\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 13s - loss: 0.1764 - acc: 0.9304 - val_loss: 0.2456 - val_acc: 0.8991\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 13s - loss: 0.1585 - acc: 0.9319 - val_loss: 0.2590 - val_acc: 0.8972\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 13s - loss: 0.1443 - acc: 0.9422 - val_loss: 0.3544 - val_acc: 0.8935\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 13s - loss: 0.1737 - acc: 0.9327 - val_loss: 0.2639 - val_acc: 0.8972\n",
      "('Train loss:', 0.11955005184192319)\n",
      "('Train accuracy:', 0.95322731602849864)\n",
      "('Test loss:', 0.23055081400915842)\n",
      "('Test accuracy:', 0.89719626212788517)\n",
      "('\\n===================FOLD=', 3)\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 16s - loss: 0.7393 - acc: 0.6238 - val_loss: 0.4565 - val_acc: 0.7790\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 13s - loss: 0.6299 - acc: 0.7066 - val_loss: 0.5146 - val_acc: 0.7341\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 13s - loss: 0.4926 - acc: 0.7498 - val_loss: 0.4283 - val_acc: 0.7790\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 13s - loss: 0.4157 - acc: 0.8022 - val_loss: 0.3180 - val_acc: 0.8596\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 13s - loss: 0.3640 - acc: 0.8330 - val_loss: 0.2954 - val_acc: 0.8577\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 13s - loss: 0.3219 - acc: 0.8546 - val_loss: 0.2556 - val_acc: 0.8933\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 13s - loss: 0.3288 - acc: 0.8460 - val_loss: 0.2847 - val_acc: 0.8783\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 13s - loss: 0.2827 - acc: 0.8741 - val_loss: 0.2516 - val_acc: 0.8914\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 13s - loss: 0.2573 - acc: 0.8837 - val_loss: 0.2265 - val_acc: 0.9064\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 13s - loss: 0.2403 - acc: 0.8930 - val_loss: 0.5341 - val_acc: 0.8202\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 13s - loss: 0.2880 - acc: 0.8705 - val_loss: 0.2578 - val_acc: 0.8895\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 13s - loss: 0.2370 - acc: 0.9051 - val_loss: 0.2730 - val_acc: 0.8801\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 13s - loss: 0.2323 - acc: 0.8977 - val_loss: 0.2434 - val_acc: 0.8951\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 13s - loss: 0.2085 - acc: 0.9148 - val_loss: 0.2450 - val_acc: 0.9082\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 13s - loss: 0.2195 - acc: 0.9145 - val_loss: 0.2360 - val_acc: 0.8951\n",
      "('Train loss:', 0.20325820457155461)\n",
      "('Train accuracy:', 0.91401869170019556)\n",
      "('Test loss:', 0.22653961628117364)\n",
      "('Test accuracy:', 0.90636704231469367)\n",
      "('\\n Train Log Loss Validation= ', 0.15293774716399414)\n",
      "(' Valid Log Loss Validation= ', 0.21748691337133552)\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"_3f_wmax\"\n",
    "train_preds , val_preds, test_preds, train_log_loss,valid_log_loss = trainKfold(X_train, X_angle, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(file=open(\"cache/{}_tmp_results.dmp\".format(exp_name),\"wb\"), obj=[train_preds , val_preds, test_preds, train_log_loss,valid_log_loss])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds , val_preds, test_preds, train_log_loss, valid_log_loss = pickle.load(file=open(\"tmp_results.dmp\",\"rb\"))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TTA_preds(exp_name):\n",
    "\n",
    "    K=3\n",
    "    y_test_pred_log = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    def gen_flow_for_two_inputs_test(test_gen, X1, X2):\n",
    "        genX2 = test_gen.flow(X1,X2, batch_size=8,shuffle=False)\n",
    "        while True:\n",
    "                X2i = genX2.next()\n",
    "                yield [X2i[0], X2i[1]]\n",
    "\n",
    "    partials = []\n",
    "    \n",
    "    \n",
    "    for j in range(K):\n",
    "        print('\\n===================FOLD=',j+1)\n",
    "        \n",
    "        model= getModel()\n",
    "\n",
    "        #Getting the Best Model\n",
    "        model.load_weights(\"weights/{}_{}.hdf5\".format(exp_name,j+1))\n",
    "        #Getting Training Score\n",
    "\n",
    "        \n",
    "        test_gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                                      vertical_flip = True, \n",
    "                                      width_shift_range = 0.,  \n",
    "                                      height_shift_range = 0.,      \n",
    "                                      channel_shift_range=0,        \n",
    "                                      zoom_range = 0.2,         \n",
    "                                      rotation_range = 10)   \n",
    "\n",
    "\n",
    "        preds = np.zeros((test.shape[0],1)).astype(np.float32) \n",
    "\n",
    "        num_aug = 5\n",
    "        for i in range(num_aug):\n",
    "            gen_flow_test = gen_flow_for_two_inputs_test(test_gen, X_test, X_test_angle)\n",
    "            preds += model.predict_generator(gen_flow_test,steps=test.shape[0]/8, verbose=1).reshape(-1,1)\n",
    "\n",
    "\n",
    "        partials.append(preds/num_aug)    \n",
    "        temp_test=preds/num_aug\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])                           \n",
    "                           \n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "\n",
    "\n",
    "    \n",
    "    return y_test_pred_log, partials\n",
    "\n",
    "\n",
    "    \n",
    "tta_preds_3fold_baseline,partials = make_TTA_preds(\"_3fold_baseline\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_results_h5(phase, exp_name, train_id, test_id, train_preds, val_preds,test_preds, train_log_loss,valid_log_loss, LB_score=0.0):\n",
    "\t\t\n",
    "\t\ttrain_preds  = pd.DataFrame(data={\"is_iceberg\":train_preds})\n",
    "\t\ttrain_preds[\"id\"]=train_id.astype(str)\n",
    "\t\ttrain_preds.set_index(\"id\",inplace=True)\n",
    "\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tval_preds  = pd.DataFrame(data={\"id\":train['id'],\"is_iceberg\":val_preds})\n",
    "\t\tval_preds[\"id\"]=train_id.astype(str)\n",
    "\t\tval_preds.set_index(\"id\",inplace=True)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tsubmission = pd.DataFrame()\n",
    "\t\tsubmission['id']=test_id\n",
    "\t\tsubmission['is_iceberg']=test_preds\n",
    "\t\tsubmission.to_csv('subm/{}.csv'.format(exp_name), index=False)\n",
    "\n",
    "\t\tsubmission['id']=test['id'].astype(str)\n",
    "\t\tsubmission.set_index(\"id\",inplace=True)\n",
    "\n",
    "\n",
    "\t\t\n",
    "\t\ttrain_preds.to_hdf('data/results.h5',\"/{}/train/{}\".format(phase,exp_name))\n",
    "\t\tval_preds.to_hdf('data/results.h5',\"/{}/valid/{}\".format(phase,exp_name))\n",
    "\t\tsubmission.to_hdf('data/results.h5',\"/{}/test/{}\".format(phase,exp_name))\n",
    "\n",
    "\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\tstore = pd.HDFStore('data/results.h5')\n",
    "\n",
    "\t\tstore.append(\"/summary\",pd.DataFrame(data={\"phase\":[phase],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"exp\":[exp_name],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"train_log_loss\":[train_log_loss],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"val_log_loss\":[valid_log_loss], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"LB\":[LB_score] }) )  \n",
    "\n",
    "\t\tstore.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_results_h5(\"ph1\", exp_name, train[\"id\"], test[\"id\"], \n",
    "                  train_preds, val_preds,test_preds, train_log_loss,valid_log_loss, LB_score=0.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds  = pd.DataFrame(data={\"is_iceberg\":train_preds})\n",
    "train_preds[\"id\"]=train['id'].astype(str)\n",
    "train_preds.set_index(\"id\",inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Submissionval_preds  = pd.DataFrame(data={\"id\":train['id'],\"is_iceberg\":val_preds})\n",
    "val_preds[\"id\"]=train['id'].astype(str)\n",
    "val_preds.set_index(\"id\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "012fc91e-17ff-4163-a32d-79007feba4fc",
    "_uuid": "2e7f1db4b36211939fb9650e3b721ac8db09dda2"
   },
   "outputs": [],
   "source": [
    "#Submission for each day.\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id'].astype(str)\n",
    "submission['is_iceberg']= tta_preds_3fold_baseline\n",
    "submission.to_csv('subm/{}.csv'.format(\"_3f_tta\"), index=False)\n",
    "\n",
    "submission.set_index(\"id\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.HDFStore('data/results.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.80484443],\n",
       "       [ 0.80484443,  1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(store.select(\"/ph1/valid/_5fold_baseline\").values.ravel(), store.select(\"/ph1/train/_5fold_baseline\").values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/summary',\n",
       " '/ph1/test/_3fold_baseline',\n",
       " '/ph1/test/_5fold_baseline',\n",
       " '/ph1/test/_5fold_fcn',\n",
       " '/ph1/train/_3fold_baseline',\n",
       " '/ph1/train/_5fold_baseline',\n",
       " '/ph1/train/_5fold_fcn',\n",
       " '/ph1/valid/_3fold_baseline',\n",
       " '/ph1/valid/_5fold_baseline',\n",
       " '/ph1/valid/_5fold_fcn']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark3cv = pd.read_csv(\"subm/sub_benchmark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark3cv.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg_3cv</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg_3cv</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>0.975071</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                is_iceberg_3cv  is_iceberg\n",
       "is_iceberg_3cv        1.000000    0.975071\n",
       "is_iceberg            0.975071    1.000000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark3cv.join(store.get(\"/ph1/test/_5fold_baseline\"),lsuffix=\"_3cv\").corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
