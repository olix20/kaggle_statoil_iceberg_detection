{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "7922e149-72fa-48cd-a545-b70bddb40d28",
    "_uuid": "9341f2516086ca38bca96e06a9dbfc39f813a95a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils \n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "9d5839b6-f7fb-426d-b05f-cf2dba9313a8",
    "_uuid": "d375d4f754ad7fb77db2142c7c075b4ad4168390"
   },
   "outputs": [],
   "source": [
    "train = pd.read_json(\"data/input/train.json\")\n",
    "target_train=train['is_iceberg']\n",
    "test = pd.read_json(\"data/input/test.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "X_angle=train['inc_angle']\n",
    "X_test_angle=test['inc_angle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xder = np.array([[-1,0,1],[-2,0,2],[-1,0,1]])\n",
    "yder = np.array([[1,2,1],[0,0,0],[-1,-2,-1]])\n",
    "smooth = np.array([[1,1,1],[1,5,1],[1,1,1]])\n",
    "xder2 = np.array([[-1,2,-1],[-3,6,-3],[-1,2,-1]])\n",
    "yder2 = np.array([[-1,-3,-1],[2,6,2],[-1,-3,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_weights = {0:}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8424, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1053.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape[0]/8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_band_3=(X_band_1+X_band_2)/2\n",
    "#X_band_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in train[\"inc_angle\"]])\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis]\n",
    "                          , X_band_2[:, :, :, np.newaxis]\n",
    "                         , X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "#X_band_test_3=np.array([np.full((75, 75), angel).astype(np.float32) for angel in test[\"inc_angle\"]])\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , X_band_test_3[:, :, :, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size=64\n",
    "# Define the image transformations here\n",
    "gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                         vertical_flip = True,\n",
    "                         width_shift_range = 0.,\n",
    "                         height_shift_range = 0.,\n",
    "                         channel_shift_range=0,\n",
    "                         zoom_range = 0.2,\n",
    "                         rotation_range = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here is the function that merges our two generators\n",
    "# We use the exact same generator with the same random seed for both the y and angle arrays\n",
    "def gen_flow_for_two_inputs(X1, X2, y):\n",
    "    genX1 = gen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    genX2 = gen.flow(X1,X2, batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            #Assert arrays are equal - this was for peace of mind, but slows down training\n",
    "            #np.testing.assert_array_equal(X1i[0],X2i[0])\n",
    "            yield [X1i[0], X2i[1]], X1i[1]\n",
    "\n",
    "# Finally create generator\n",
    "def get_callbacks(filepath, patience=5):\n",
    "    es = EarlyStopping('val_loss', patience=5, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "#     lrplateu = ReduceLROnPlateau(monitor='val_loss',  patience=2, verbose=1, factor=0.5, min_lr=1e-6)\n",
    "\n",
    "    return [es, msave]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    input_2 = Input(shape=[1], name=\"angle\")\n",
    "    angle_layer = Dense(1, )(input_2)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, \n",
    "                 input_shape=X_train.shape[1:], classes=1)\n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "\n",
    "    x = Flatten()(x) #GlobalMaxPooling2D\n",
    "    merge_one = concatenate([x, angle_layer])\n",
    "    merge_one = Dense(512, activation='relu', name='fc2')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    merge_one = Dense(512, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=[base_model.input, input_2], output=predictions)\n",
    "    \n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "067f3dd7-3dcf-4b71-857d-e00b4afbd06e",
    "_uuid": "af8be6ce23dba815bbde23fd7e196eb54ae7c4e1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Using K-fold Cross Validation with Data Augmentation.\n",
    "def trainKfold(X_train, X_angle, X_test):\n",
    "    K=3\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=17).split(X_train, target_train))\n",
    "    y_test_pred_log = 0\n",
    "    y_train_pred_log=0\n",
    "    y_valid_pred_log = 0.0*target_train\n",
    "    \n",
    "    \n",
    "    for j, (train_idx, test_idx) in enumerate(folds):\n",
    "        print('\\n===================FOLD=',j+1)\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = target_train[train_idx]\n",
    "        X_holdout = X_train[test_idx]\n",
    "        Y_holdout= target_train[test_idx]\n",
    "        \n",
    "        #Angle\n",
    "        X_angle_cv=X_angle[train_idx]\n",
    "        X_angle_hold=X_angle[test_idx]\n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"weights/{}_{}.hdf5\".format(exp_name,j+1)\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_flow = gen_flow_for_two_inputs(X_train_cv, X_angle_cv, y_train_cv)\n",
    "        \n",
    "        model= getModel()\n",
    "        model.fit_generator(\n",
    "                gen_flow,\n",
    "                steps_per_epoch=24,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=([X_holdout,X_angle_hold], Y_holdout),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        model.load_weights(filepath=file_path)\n",
    "        #Getting Training Score\n",
    "        score = model.evaluate([X_train_cv,X_angle_cv], y_train_cv, verbose=0)\n",
    "        print('Train loss:', score[0])\n",
    "        print('Train accuracy:', score[1])\n",
    "        \n",
    "        #Getting Test Score\n",
    "        score = model.evaluate([X_holdout,X_angle_hold], Y_holdout, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "\n",
    "        #Getting validation Score.\n",
    "        pred_valid=model.predict([X_holdout,X_angle_hold])\n",
    "        y_valid_pred_log[test_idx] = pred_valid.reshape(pred_valid.shape[0])\n",
    "\n",
    "        #Getting Test Scores\n",
    "\n",
    "        temp_test=model.predict([X_test, X_test_angle])\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])\n",
    "        \n",
    "\n",
    "\n",
    "        #Getting Train Scores\n",
    "        temp_train=model.predict([X_train, X_angle])\n",
    "        y_train_pred_log+=temp_train.reshape(temp_train.shape[0])\n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "    y_train_pred_log=y_train_pred_log/K\n",
    "\n",
    "    \n",
    "    train_log_loss = log_loss(target_train, y_train_pred_log) \n",
    "    valid_log_loss = log_loss(target_train, y_valid_pred_log)\n",
    "    \n",
    "    print('\\n Train Log Loss Validation= ',train_log_loss)\n",
    "    print(' Valid Log Loss Validation= ',valid_log_loss)\n",
    "    \n",
    "    \n",
    "    return y_train_pred_log, y_valid_pred_log, y_test_pred_log, train_log_loss, valid_log_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "ea82458f-f41c-4abb-87aa-0dfc7a447969",
    "_uuid": "d462c689ee61d4c1cdcee42c7ded6c7c31c9cddc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n===================FOLD=', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:18: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 14s - loss: 0.6536 - acc: 0.6456 - val_loss: 0.6366 - val_acc: 0.6935\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 13s - loss: 0.4052 - acc: 0.8087 - val_loss: 0.2948 - val_acc: 0.8673\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 13s - loss: 0.3521 - acc: 0.8407 - val_loss: 0.2642 - val_acc: 0.8785\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 13s - loss: 0.3089 - acc: 0.8667 - val_loss: 0.2450 - val_acc: 0.8879\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 13s - loss: 0.2644 - acc: 0.8752 - val_loss: 0.2261 - val_acc: 0.9047\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 13s - loss: 0.2591 - acc: 0.8838 - val_loss: 0.2233 - val_acc: 0.8916\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 13s - loss: 0.2524 - acc: 0.8891 - val_loss: 0.2190 - val_acc: 0.9028\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 13s - loss: 0.2417 - acc: 0.9073 - val_loss: 0.2189 - val_acc: 0.9084\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 13s - loss: 0.2183 - acc: 0.9044 - val_loss: 0.2064 - val_acc: 0.9140\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 13s - loss: 0.2121 - acc: 0.9142 - val_loss: 0.2136 - val_acc: 0.9065\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 13s - loss: 0.1958 - acc: 0.9152 - val_loss: 0.2053 - val_acc: 0.9140\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 13s - loss: 0.2127 - acc: 0.9128 - val_loss: 0.2605 - val_acc: 0.8897\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 13s - loss: 0.2007 - acc: 0.9155 - val_loss: 0.1987 - val_acc: 0.9140\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 13s - loss: 0.1943 - acc: 0.9140 - val_loss: 0.2081 - val_acc: 0.9140\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 13s - loss: 0.1732 - acc: 0.9263 - val_loss: 0.2008 - val_acc: 0.9196\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 13s - loss: 0.1677 - acc: 0.9308 - val_loss: 0.2345 - val_acc: 0.9159\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 13s - loss: 0.1928 - acc: 0.9198 - val_loss: 0.2174 - val_acc: 0.9178\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 13s - loss: 0.1644 - acc: 0.9321 - val_loss: 0.2108 - val_acc: 0.9103\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 13s - loss: 0.1628 - acc: 0.9296 - val_loss: 0.2761 - val_acc: 0.8916\n",
      "('Train loss:', 0.14312380611185607)\n",
      "('Train accuracy:', 0.9448082319925164)\n",
      "('Test loss:', 0.19867231667598831)\n",
      "('Test accuracy:', 0.91401869125455337)\n",
      "('\\n===================FOLD=', 2)\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 14s - loss: 0.7007 - acc: 0.6186 - val_loss: 0.3721 - val_acc: 0.8187\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 13s - loss: 0.4552 - acc: 0.7768 - val_loss: 0.3135 - val_acc: 0.8486\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 13s - loss: 0.3314 - acc: 0.8421 - val_loss: 0.3196 - val_acc: 0.8561\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 13s - loss: 0.3106 - acc: 0.8545 - val_loss: 0.2671 - val_acc: 0.8766\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 13s - loss: 0.2821 - acc: 0.8722 - val_loss: 0.2887 - val_acc: 0.8673\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 13s - loss: 0.2451 - acc: 0.8922 - val_loss: 0.2832 - val_acc: 0.8598\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 13s - loss: 0.2614 - acc: 0.8823 - val_loss: 0.2981 - val_acc: 0.8617\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 13s - loss: 0.2360 - acc: 0.8931 - val_loss: 0.2383 - val_acc: 0.8953\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 13s - loss: 0.2109 - acc: 0.9148 - val_loss: 0.2660 - val_acc: 0.8897\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 13s - loss: 0.2184 - acc: 0.9172 - val_loss: 0.2565 - val_acc: 0.8935\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 13s - loss: 0.1996 - acc: 0.9204 - val_loss: 0.3084 - val_acc: 0.8505\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 13s - loss: 0.2168 - acc: 0.9069 - val_loss: 0.2508 - val_acc: 0.8841\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 13s - loss: 0.2017 - acc: 0.9121 - val_loss: 0.2551 - val_acc: 0.8897\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 13s - loss: 0.2022 - acc: 0.9152 - val_loss: 0.2427 - val_acc: 0.8935\n",
      "('Train loss:', 0.17232511456710808)\n",
      "('Train accuracy:', 0.93171188109828784)\n",
      "('Test loss:', 0.23830526103483182)\n",
      "('Test accuracy:', 0.89532710369502277)\n",
      "('\\n===================FOLD=', 3)\n",
      "Epoch 1/100\n",
      "24/24 [==============================] - 14s - loss: 0.6279 - acc: 0.6607 - val_loss: 0.3719 - val_acc: 0.8258\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 13s - loss: 0.3889 - acc: 0.8063 - val_loss: 0.3022 - val_acc: 0.8577\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 13s - loss: 0.3380 - acc: 0.8541 - val_loss: 0.2576 - val_acc: 0.8914\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 13s - loss: 0.2948 - acc: 0.8481 - val_loss: 0.2465 - val_acc: 0.8914\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 13s - loss: 0.2657 - acc: 0.8855 - val_loss: 0.2542 - val_acc: 0.8951\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 13s - loss: 0.2708 - acc: 0.8902 - val_loss: 0.2354 - val_acc: 0.9026\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 13s - loss: 0.2636 - acc: 0.8823 - val_loss: 0.2541 - val_acc: 0.8876\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 13s - loss: 0.2389 - acc: 0.8939 - val_loss: 0.2354 - val_acc: 0.9026\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 13s - loss: 0.2228 - acc: 0.9044 - val_loss: 0.2174 - val_acc: 0.9139\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 13s - loss: 0.1990 - acc: 0.9128 - val_loss: 0.2542 - val_acc: 0.9064\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 13s - loss: 0.1995 - acc: 0.9211 - val_loss: 0.2867 - val_acc: 0.8876\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 13s - loss: 0.1928 - acc: 0.9201 - val_loss: 0.2583 - val_acc: 0.8951\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 13s - loss: 0.1824 - acc: 0.9203 - val_loss: 0.2621 - val_acc: 0.9101\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 13s - loss: 0.1726 - acc: 0.9317 - val_loss: 0.2443 - val_acc: 0.9120\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 13s - loss: 0.1833 - acc: 0.9269 - val_loss: 0.2465 - val_acc: 0.9007\n",
      "('Train loss:', 0.16259401538104654)\n",
      "('Train accuracy:', 0.93551401913723098)\n",
      "('Test loss:', 0.21741728914364447)\n",
      "('Test accuracy:', 0.91385767968852871)\n",
      "('\\n Train Log Loss Validation= ', 0.16552069651106005)\n",
      "(' Valid Log Loss Validation= ', 0.2181320649412769)\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"_3fmax_tflat\"\n",
    "train_preds , val_preds, test_preds, train_log_loss,valid_log_loss = trainKfold(X_train, X_angle, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(file=open(\"cache/{}_tmp_results.dmp\".format(exp_name),\"wb\"), obj=[train_preds , val_preds, test_preds, train_log_loss,valid_log_loss])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds , val_preds, test_preds, train_log_loss, valid_log_loss = pickle.load(file=open(\"tmp_results.dmp\",\"rb\"))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TTA_preds(exp_name):\n",
    "\n",
    "    K=3\n",
    "    y_test_pred_log = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    def gen_flow_for_two_inputs_test(test_gen, X1, X2):\n",
    "        genX2 = test_gen.flow(X1,X2, batch_size=8,shuffle=False)\n",
    "        while True:\n",
    "                X2i = genX2.next()\n",
    "                yield [X2i[0], X2i[1]]\n",
    "\n",
    "    partials = []\n",
    "    \n",
    "    \n",
    "    for j in range(K):\n",
    "        print('\\n===================FOLD=',j+1)\n",
    "        \n",
    "        model= getModel()\n",
    "\n",
    "        #Getting the Best Model\n",
    "        model.load_weights(\"weights/{}_{}.hdf5\".format(exp_name,j+1))\n",
    "        #Getting Training Score\n",
    "\n",
    "        \n",
    "        test_gen = ImageDataGenerator(horizontal_flip = True,\n",
    "                                      vertical_flip = True, \n",
    "                                      width_shift_range = 0.,  \n",
    "                                      height_shift_range = 0.,      \n",
    "                                      channel_shift_range=0,        \n",
    "                                      zoom_range = 0.2,         \n",
    "                                      rotation_range = 10)   \n",
    "\n",
    "\n",
    "        preds = np.zeros((test.shape[0],1)).astype(np.float32) \n",
    "\n",
    "        num_aug = 5\n",
    "        for i in range(num_aug):\n",
    "            gen_flow_test = gen_flow_for_two_inputs_test(test_gen, X_test, X_test_angle)\n",
    "            preds += model.predict_generator(gen_flow_test,steps=test.shape[0]/8, verbose=1).reshape(-1,1)\n",
    "\n",
    "\n",
    "        partials.append(preds/num_aug)    \n",
    "        temp_test=preds/num_aug\n",
    "        y_test_pred_log+=temp_test.reshape(temp_test.shape[0])                           \n",
    "                           \n",
    "\n",
    "    y_test_pred_log=y_test_pred_log/K\n",
    "\n",
    "\n",
    "    \n",
    "    return y_test_pred_log, partials\n",
    "\n",
    "\n",
    "    \n",
    "tta_preds_3fold_baseline,partials = make_TTA_preds(\"_3fold_baseline\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_results_h5(phase, exp_name, train_id, test_id, train_preds, val_preds,test_preds, train_log_loss,valid_log_loss, LB_score=0.0):\n",
    "\t\t\n",
    "\t\ttrain_preds  = pd.DataFrame(data={\"is_iceberg\":train_preds})\n",
    "\t\ttrain_preds[\"id\"]=train_id.astype(str)\n",
    "\t\ttrain_preds.set_index(\"id\",inplace=True)\n",
    "\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tval_preds  = pd.DataFrame(data={\"id\":train['id'],\"is_iceberg\":val_preds})\n",
    "\t\tval_preds[\"id\"]=train_id.astype(str)\n",
    "\t\tval_preds.set_index(\"id\",inplace=True)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tsubmission = pd.DataFrame()\n",
    "\t\tsubmission['id']=test_id\n",
    "\t\tsubmission['is_iceberg']=test_preds\n",
    "\t\tsubmission.to_csv('subm/{}.csv'.format(exp_name), index=False)\n",
    "\n",
    "\t\tsubmission['id']=test['id'].astype(str)\n",
    "\t\tsubmission.set_index(\"id\",inplace=True)\n",
    "\n",
    "\n",
    "\t\t\n",
    "\t\ttrain_preds.to_hdf('data/results.h5',\"/{}/train/{}\".format(phase,exp_name))\n",
    "\t\tval_preds.to_hdf('data/results.h5',\"/{}/valid/{}\".format(phase,exp_name))\n",
    "\t\tsubmission.to_hdf('data/results.h5',\"/{}/test/{}\".format(phase,exp_name))\n",
    "\n",
    "\n",
    "\n",
    "\t\t\n",
    "\n",
    "\t\tstore = pd.HDFStore('data/results.h5')\n",
    "\n",
    "\t\tstore.append(\"/summary\",pd.DataFrame(data={\"phase\":[phase],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"exp\":[exp_name],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"train_log_loss\":[train_log_loss],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"val_log_loss\":[valid_log_loss], \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \"LB\":[LB_score] }) )  \n",
    "\n",
    "\t\tstore.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_results_h5(\"ph1\", exp_name, train[\"id\"], test[\"id\"], \n",
    "                  train_preds, val_preds,test_preds, train_log_loss,valid_log_loss, LB_score=0.176 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds  = pd.DataFrame(data={\"is_iceberg\":train_preds})\n",
    "train_preds[\"id\"]=train['id'].astype(str)\n",
    "train_preds.set_index(\"id\",inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Submissionval_preds  = pd.DataFrame(data={\"id\":train['id'],\"is_iceberg\":val_preds})\n",
    "val_preds[\"id\"]=train['id'].astype(str)\n",
    "val_preds.set_index(\"id\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "012fc91e-17ff-4163-a32d-79007feba4fc",
    "_uuid": "2e7f1db4b36211939fb9650e3b721ac8db09dda2"
   },
   "outputs": [],
   "source": [
    "#Submission for each day.\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id'].astype(str)\n",
    "submission['is_iceberg']= tta_preds_3fold_baseline\n",
    "submission.to_csv('subm/{}.csv'.format(\"_3f_tta\"), index=False)\n",
    "\n",
    "submission.set_index(\"id\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.HDFStore('data/results.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.80484443],\n",
       "       [ 0.80484443,  1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(store.select(\"/ph1/valid/_5fold_baseline\").values.ravel(), store.select(\"/ph1/train/_5fold_baseline\").values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/summary',\n",
       " '/ph1/test/_3fold_baseline',\n",
       " '/ph1/test/_5fold_baseline',\n",
       " '/ph1/test/_5fold_fcn',\n",
       " '/ph1/train/_3fold_baseline',\n",
       " '/ph1/train/_5fold_baseline',\n",
       " '/ph1/train/_5fold_fcn',\n",
       " '/ph1/valid/_3fold_baseline',\n",
       " '/ph1/valid/_5fold_baseline',\n",
       " '/ph1/valid/_5fold_fcn']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark3cv = pd.read_csv(\"subm/sub_benchmark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark3cv.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_iceberg_3cv</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is_iceberg_3cv</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_iceberg</th>\n",
       "      <td>0.975071</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                is_iceberg_3cv  is_iceberg\n",
       "is_iceberg_3cv        1.000000    0.975071\n",
       "is_iceberg            0.975071    1.000000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark3cv.join(store.get(\"/ph1/test/_5fold_baseline\"),lsuffix=\"_3cv\").corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
