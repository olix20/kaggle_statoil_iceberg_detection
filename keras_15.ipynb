{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c825acee-1835-4fa0-a14a-0c40a589577a",
    "_uuid": "a84233f5cbe98d402d74f46e87e7d745540b2cc8"
   },
   "source": [
    "This Kernal implements a Keras + Tensorflow CNN for the StatOil Iceberg competition. It has yielded results of 0.1995 on the leaderboard. With some tuning and image filtering plus more of an inclusion of the incident angle, a better result could be yielded I'm sure.\n",
    "\n",
    "The input is a 75x75x3 set of images. The output is a binary 0/1 where 1 is noteed as an iceberg. \n",
    "\n",
    "The set of images are band_1 (HH), band_2 (HV), and an combined band which would be (HH dot HV)/constant. However, since we are working with the images in dB, the 3rd band is modified to compenate for the log function yielding band_1 + band_2 -log(constant). The last term is neglected as when the images are scaled the 3rd term would be removed by the mathematics anyway.\n",
    "\n",
    "This and other information can be found from: https://earth.esa.int/c/document_library/get_file?folderId=409229&name=DLFE-5566.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3742280f-b7f8-4639-a1dc-fe34c5d0bbe5",
    "_uuid": "fb4f08cce0aaddbda853b5c86f8b66e6189b10a4"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "06ed8576-67b0-407a-bec3-a8919432eb4b",
    "_uuid": "a6025efc05dbb5d0a9f30e1b70e6178a52f7bb5b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import cv2 # Used to manipulated the images \n",
    "np.random.seed(1337) # The seed I used - pick your own or comment out for a random seed. A constant seed allows for better comparisons though\n",
    "\n",
    "# Import Keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4d270cfb-b795-4fcb-a9a1-036d76df11cd",
    "_uuid": "edd393b24524487da40e6afded38bf69420e4c0c"
   },
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3929b5f5-42ae-4129-be7d-bb62ef9a0892",
    "_uuid": "8b4c8df78f70528e640d2fa67224d81122f19c09",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_json('data/input/train.json') # this is a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "acd72dcd-7a59-4b81-9181-b03c96d33698",
    "_uuid": "fc90560cb8c46a84df31a27f202521f174393da0"
   },
   "source": [
    "Need to reshape and feature scale the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "715ebc9b-0999-456e-8ed0-bc96e05e0128",
    "_uuid": "2782430813d6deda5a6f8aae1cc385586538f5c5"
   },
   "outputs": [],
   "source": [
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "85302ec3-62ea-428b-94d1-4e5cfce6636a",
    "_uuid": "5c5e277b79272ef634783caf1fe8aa16447d6a3a"
   },
   "outputs": [],
   "source": [
    "Xtrain = get_scaled_imgs(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c82bf9e6-0037-451d-9ca5-e6ff8d308e5c",
    "_uuid": "fd2fd9004992dbf7423c7c3f2b6307e82e99cb43"
   },
   "source": [
    "Get the response variable \"is_iceberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "cef1dcf0-a325-42f9-b67c-e6cecd8b905f",
    "_uuid": "3d86dd956ad0c36ce3ad1b7b3f498478b44c93b4"
   },
   "outputs": [],
   "source": [
    "Ytrain = np.array(df_train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "969b8bf1-97ff-43a5-8559-de6b30e7f067",
    "_uuid": "5e26d33941b02e8fd5f9447ffa81643a94b4b47c"
   },
   "source": [
    "Some of the incident angle from the satellite are unknown and marked as \"na\". Replace these na with 0 and find the indices where the incident angle is >0 (this way you can use a truncated set or the full set of training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "8cd73730-f3ac-4aac-8abd-1908db72aa14",
    "_uuid": "5e9a904a1dab28585c117f6fc751fee3297b95b5"
   },
   "outputs": [],
   "source": [
    "df_train.inc_angle = df_train.inc_angle.replace('na',0)\n",
    "idx_tr = np.where(df_train.inc_angle>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec8be8e9-66aa-4952-8e85-25a425146e04",
    "_uuid": "a6685ee2d0e7f58bb5f0ec4c2ed7b85451edd384"
   },
   "source": [
    "You can now use the option of training with only known incident angles or the whole set. I found slightly better results training with only the known incident angles so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "1b90f6f0-2cf7-4cfd-8186-791c7ac7122f",
    "_uuid": "9f34935b2730a0ba9748916a278d14b4eb76b1c0"
   },
   "outputs": [],
   "source": [
    "Ytrain = Ytrain[idx_tr[0]]\n",
    "Xtrain = Xtrain[idx_tr[0],...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "306fe2f3-f417-4f2d-956b-2cc98c8b955b",
    "_uuid": "8da34b46d1f4c96d9f32239df3b61573cde9d0e1"
   },
   "source": [
    "## Adding images for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "49117bf4-dbdd-4ea5-95ca-af23ed35053e",
    "_uuid": "fba1fe217316a5ca70887c67d2c0c158cb8c02dc"
   },
   "source": [
    "Now, the biggest improvement I had was by adding more data to train on. I did this by simply including horizontally and vertically flipped data. Using OpenCV this is easily done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "f069618d-46f0-42f0-8e83-e5a4716bb965",
    "_uuid": "760fa192e912111b39d3d0ee3fadd9c7fd0eb628"
   },
   "outputs": [],
   "source": [
    "def get_more_images(imgs):\n",
    "    \n",
    "    more_images = []\n",
    "    vert_flip_imgs = []\n",
    "    hori_flip_imgs = []\n",
    "      \n",
    "    for i in range(0,imgs.shape[0]):\n",
    "        a=imgs[i,:,:,0]\n",
    "        b=imgs[i,:,:,1]\n",
    "        c=imgs[i,:,:,2]\n",
    "        \n",
    "        av=cv2.flip(a,1)\n",
    "        ah=cv2.flip(a,0)\n",
    "        bv=cv2.flip(b,1)\n",
    "        bh=cv2.flip(b,0)\n",
    "        cv=cv2.flip(c,1)\n",
    "        ch=cv2.flip(c,0)\n",
    "        \n",
    "        vert_flip_imgs.append(np.dstack((av, bv, cv)))\n",
    "        hori_flip_imgs.append(np.dstack((ah, bh, ch)))\n",
    "      \n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(hori_flip_imgs)\n",
    "       \n",
    "    more_images = np.concatenate((imgs,v,h))\n",
    "    \n",
    "    return more_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "31907cfa-6fdd-400d-ba9e-b755c40c7630",
    "_uuid": "39719b4ceb5cbfc4bf94461d3108e77f3688ad84"
   },
   "source": [
    "I rename the returned value so i have the option of using the original data set or the expanded data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "f5cf48ee-7f83-44c0-9d13-61d492a27428",
    "_uuid": "bbab6fcb834b8f5eaf9f56ecef2d068df617b749"
   },
   "outputs": [],
   "source": [
    "Xtr_more = get_more_images(Xtrain) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "58261f5c-de26-41f5-805b-2fec1fabffef",
    "_uuid": "99e06106fcbf3b642f2e7f7c63da33a356a09157"
   },
   "source": [
    "And then define the new response variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "3c8be0d9-9dee-45e8-affc-7ad95c30e111",
    "_uuid": "f735f520aab4cfd64ae9e0a098aa441372c5b8b7"
   },
   "outputs": [],
   "source": [
    "Ytr_more = np.concatenate((Ytrain,Ytrain,Ytrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54a72cfc-dbc1-4136-bb31-2ea75115e1cf",
    "_uuid": "f2b22fcec3493ce4df71f0dd21ed97fd1938e3f7"
   },
   "source": [
    "## CNN Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "60ecc07f-3dc5-4ba6-9c02-127ebec699a7",
    "_uuid": "55f2def478e9adf2293cc0362a96f65884e7eab8"
   },
   "source": [
    "Now the nitty gritty of the situation, the CNN model. This is a simplistic model that should give reasonable results. It is not tuned that well and there are plenty of options and changes you can try so as to improve it. At least you will get the idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "c3d1f84c-44e0-45df-9363-13f5b8abd6ef",
    "_uuid": "edd8ce229d5914f9f57bf147dccb4810c06dbf09"
   },
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    #Build keras model\n",
    "    \n",
    "    model=Sequential()\n",
    "    # CNN 1\n",
    "    model.add(BatchNormalization(input_shape=(75, 75, 3)))\n",
    "    model.add(Conv2D(64, kernel_size=(5, 5)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "\n",
    "    # CNN 2\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # CNN 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))\n",
    "\n",
    "    # CNN 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(GlobalMaxPooling2D())\n",
    "\n",
    "#     #CNN 4\n",
    "#     model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Activation('relu'))    \n",
    "    \n",
    "    \n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # You must flatten the data for the dense layers\n",
    "#     model.add(Flatten())\n",
    "\n",
    "    #Dense 1\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "    #Dense 2\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0fdf0c92-bf4f-45a8-b764-8f29c20f8b99",
    "_uuid": "14fe4ef731890428d6ee9616e04339829c94d6e9"
   },
   "source": [
    "Now get the model and get ready to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "c8d9002e-8166-4af4-93f9-fe1128504415",
    "_uuid": "26dcf890e933e3a088c2f3bcbd1c8528fbbccca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_90 (Batc (None, 75, 75, 3)         12        \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 71, 71, 64)        4864      \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 71, 71, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 71, 71, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 69, 69, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 69, 69, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 69, 69, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_81 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 30, 30, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 30, 30, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 13, 13, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 13, 13, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_84 (Conv2D)           (None, 11, 11, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 11, 11, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_85 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 6, 6, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_86 (Activation)   (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_8 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 873,805\n",
      "Trainable params: 872,007\n",
      "Non-trainable params: 1,798\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel()\n",
    "model.summary()\n",
    "\n",
    "batch_size = 64\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('weights/keras_15.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-3, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c18eef26-051b-412a-a13b-bf26f416e830",
    "_uuid": "6dbc30eedfb3fafad4dc19f68c41ba6ac3f5e421"
   },
   "source": [
    "Now train the model! (Each epoch ran at about 10s on GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "06dcfa2a-2742-4095-893d-f9b578558879",
    "_uuid": "e659b7bc60a664039514bb043910d55a82d2f5fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3309 samples, validate on 1104 samples\n",
      "Epoch 1/100\n",
      "3309/3309 [==============================] - 19s 6ms/step - loss: 0.3759 - acc: 0.8202 - val_loss: 7.3946 - val_acc: 0.4937\n",
      "Epoch 2/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.2326 - acc: 0.8997 - val_loss: 8.1612 - val_acc: 0.4937\n",
      "Epoch 3/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.1987 - acc: 0.9178 - val_loss: 8.1612 - val_acc: 0.4937\n",
      "Epoch 4/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.1924 - acc: 0.9223 - val_loss: 8.1612 - val_acc: 0.4937\n",
      "Epoch 5/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.1603 - acc: 0.9350 - val_loss: 7.8599 - val_acc: 0.4937\n",
      "Epoch 6/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.1568 - acc: 0.9353 - val_loss: 8.1430 - val_acc: 0.4937\n",
      "Epoch 7/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.1307 - acc: 0.9447 - val_loss: 4.5125 - val_acc: 0.4937\n",
      "Epoch 8/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0707 - acc: 0.9731 - val_loss: 0.9873 - val_acc: 0.6966\n",
      "Epoch 9/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0760 - acc: 0.9707 - val_loss: 1.0513 - val_acc: 0.7636\n",
      "Epoch 10/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0951 - acc: 0.9643 - val_loss: 1.5403 - val_acc: 0.7156\n",
      "Epoch 11/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0660 - acc: 0.9749 - val_loss: 0.5336 - val_acc: 0.8388\n",
      "Epoch 12/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0567 - acc: 0.9782 - val_loss: 0.3473 - val_acc: 0.9058\n",
      "Epoch 13/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0579 - acc: 0.9761 - val_loss: 0.4060 - val_acc: 0.8786\n",
      "Epoch 14/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0383 - acc: 0.9873 - val_loss: 0.4769 - val_acc: 0.8986\n",
      "Epoch 15/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0304 - acc: 0.9870 - val_loss: 0.4490 - val_acc: 0.9031\n",
      "Epoch 16/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0272 - acc: 0.9912 - val_loss: 0.7238 - val_acc: 0.8524\n",
      "Epoch 17/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0380 - acc: 0.9843 - val_loss: 0.5092 - val_acc: 0.8714\n",
      "Epoch 18/100\n",
      "3264/3309 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9838\n",
      "Epoch 00018: reducing learning rate to 0.00010000000475.\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0442 - acc: 0.9828 - val_loss: 1.0214 - val_acc: 0.7808\n",
      "Epoch 19/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0230 - acc: 0.9924 - val_loss: 0.5222 - val_acc: 0.8895\n",
      "Epoch 20/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0070 - acc: 0.9988 - val_loss: 0.3800 - val_acc: 0.9022\n",
      "Epoch 21/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0050 - acc: 0.9994 - val_loss: 0.3480 - val_acc: 0.9176\n",
      "Epoch 22/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0037 - acc: 0.9994 - val_loss: 0.3458 - val_acc: 0.9194\n",
      "Epoch 23/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0027 - acc: 0.9997 - val_loss: 0.3441 - val_acc: 0.9176\n",
      "Epoch 24/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.3420 - val_acc: 0.9203\n",
      "Epoch 25/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.3497 - val_acc: 0.9194\n",
      "Epoch 26/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.3544 - val_acc: 0.9203\n",
      "Epoch 27/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.3760 - val_acc: 0.9203\n",
      "Epoch 28/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3701 - val_acc: 0.9221\n",
      "Epoch 29/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3687 - val_acc: 0.9221\n",
      "Epoch 30/100\n",
      "3264/3309 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 00030: reducing learning rate to 1.0000000475e-05.\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0023 - acc: 0.9997 - val_loss: 0.3784 - val_acc: 0.9194\n",
      "Epoch 31/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.3775 - val_acc: 0.9194\n",
      "Epoch 32/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0024 - acc: 0.9997 - val_loss: 0.3768 - val_acc: 0.9203\n",
      "Epoch 33/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 8.2836e-04 - acc: 1.0000 - val_loss: 0.3771 - val_acc: 0.9203\n",
      "Epoch 34/100\n",
      "3309/3309 [==============================] - 13s 4ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.3776 - val_acc: 0.9221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2098f5a190>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtr_more, Ytr_more, batch_size=batch_size, epochs=100, verbose=1, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4218bfc9-ab3f-4d21-85d6-f94d0d7d8546",
    "_uuid": "9d24cf0fc1c16a191a292cc2b2f980a09cd024d0"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a9ed1ef9-5c02-4ad3-b194-87ac66abe186",
    "_uuid": "5b318d35d321fb086abff8bf0127bd7f4e8bffd4"
   },
   "source": [
    "Load the best weights and check the score on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "4ba686cb-4b3e-4f56-b9d9-bad79ab28689",
    "_uuid": "7c362819ea7e89e54ecabfc7d17b497f0d06b764"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1471/1471 [==============================] - 11s 7ms/step\n",
      "Train score: 0.122238973876\n",
      "Train accuracy: 0.960571040109\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath = '.mdl_wts.hdf5')\n",
    "\n",
    "score = model.evaluate(Xtrain, Ytrain, verbose=1)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "70050e6f-45e0-4c3f-8c8e-585dd69b8c4e",
    "_uuid": "c4c25cb093912bc6ee406d2698bd71822e6dd23c"
   },
   "source": [
    "Now, to make a submission, load the test data and train the model and output a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "7a64efcf-3e7b-472f-806c-1d90e09b2228",
    "_uuid": "2892d23d42f81ea2e61898da0f01ea68cd1f0f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  is_iceberg\n",
      "0  5941774d    0.015196\n",
      "1  4023181e    0.939407\n",
      "2  b20200e4    0.040264\n",
      "3  e7f018bb    0.999260\n",
      "4  4371c8c3    0.828124\n",
      "5  a8d9b1fd    0.055037\n",
      "6  29e7727e    0.057872\n",
      "7  92a51ffb    0.999640\n",
      "8  c769ac97    0.000258\n",
      "9  aee0547d    0.000067\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_json('../input/test.json')\n",
    "df_test.inc_angle = df_test.inc_angle.replace('na',0)\n",
    "Xtest = (get_scaled_imgs(df_test))\n",
    "pred_test = model.predict(Xtest)\n",
    "\n",
    "submission = pd.DataFrame({'id': df_test[\"id\"], 'is_iceberg': pred_test.reshape((pred_test.shape[0]))})\n",
    "print(submission.head(10))\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a760f72b-becb-47ae-8184-d513ea622f75",
    "_uuid": "5a789e5503e9c99ea771347637593f2f1b32f0e1"
   },
   "source": [
    "The best submission with this I received was 0.1995 on the leaderboard. Have a go and see how well you can do!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
